## Introduction

"justification scientifique" pour le racisme et l'esclavage ?
	-> Samuel Morton et classification de races humaines selon les caractéristiques des crânes
	
Problème avec ses travaux : il choisissait des données qui supportaient ses opinions (white supremacist ideology)
	-> cependant, même si son travail n'était pas "légitime", il a conduit à des hiérarchies  discriminatoires

**La même chose s'applique à l'IA :** la classification va définir la manière dont "pense" l'IA 

**Quelles théories sociales et politiques sur la classification ?**
**Comment la classification renforce-t-elle le pouvoir et les inégalités ?**

## IA biaisée ?

Algorithmes de classification : mène à des systèmes d'IA discriminatoires et biaisés vis-à-vis des genres et des ethnies
	-> exemple : IA crée par Amazon pour automatiquement inspecter les CVs de candidats et trouver les meilleurs. Problème : le dataset choisi était celui des CVs des employés d'Amazon des X dernières années, donc il y avait principalement des hommes. L'IA ne recommandait donc jamais de femmes
Cependant, pas si simple "d'enlever" le fait qu'un modèle soit biaisé : dans le cas d'Amazon, même si le modèle ne prenait plus en compte le genre, il le distinguait dans  la façon d'écrire et ne recommandait donc toujours pas de femmes

## Limites de la "débiaisation"

IBM voulaient débiaiser un système de reconnaissance faciale
	-> pour ça, ils ont pris un grand volume de visages divers
Problème : IBM se concentrait sur les mesures crâniennes et la classification binaire du genre
	-> ignorant donc la possibilité de non-binaires et de transgenres
L'approche d'IBM est donc critiquée car elle est concentrée sur les traits physiques et ne prend pas en compte les identités culturelles ou sociales de la personne
	-> De plus, les méthodes ne sont pas nuancées. Une méthode de reconnaissance d'identité (tel que la reconnaissance de couleur de peau) est faite non pas parce qu'elle donne des informations importantes sur la culture ou la diversité, mais parce que c'est une méthode aisément réalisable

## Définition d'un biais (préjugé)

En statistiques, un bias désigne une imprécision/mauvaise représentation systématique de la réalité

Définition adoptée pour la classification en machine learning : bias comme en statistiques, mais dans le processus de prédiction. Cette définition est + vue comme des erreurs techniques

## Classification *volontaire* ?

Ces préjugés ne sont pas (que) des erreurs techniques
	-> chaque dataset en ML contient une vision du monde façonnée par des choix politiques, culturels et sociaux

**Comment sont faites ces classifications et comment impactent elles les structures sociales ?**

## L'exemple d'ImageNet

ImageNet : dataset de 15 millions d'image annotées
Taxonomie (= diversité du monde vivant) d'ImageNet
	-> catégories telles que plante, objet naturel, personne, animal, ...
		-> dans tout domaine (professionnel, amateur, religieux, ...)

#### Préjugés dans le système de classification d'ImageNet

-> classification humaine = normes sociétales et préjugés (catégorisation de genre, sexualité)
	-> classification de genre : **binaire** (non-binaires et transgenres rendus invisibles)
		-> catégorisations historiques ? Telles que l'homosexualité comme un trouble mental

 **Problème notable :** ImageNet associe des labels à des individus, sans leur consentement

**Effet de cycle :** de telles classifications  naturalisent des catégories particulières du monde qui produisent des effets qui sont perçus comme les justifiant

#### Plein de stéréotypes

**ImageNet :** 2 832 sous catégories de "Personne"
	-> gal, grandfather, dad, CEO

ImageNet est plein de stéréotypes
	-> Catégories de "Personne" divisées par 2 après que des étudiants soient chargés d'enlever toute catégorie offensive ou sensible

Au delà des catégories stéréotypées, les catégories dit neutres reflètent des hypothèses et des dynamiques de pouvoir

#### Origine des images "Personne"

-> depuis des moteurs de recherche comme Google, sans le consentement des gens sur les photos -> Mechanical Turk (50 images à labelliser par minute)

ImageNet est un bon exemple d'images obtenues à travers l'exploitation de main d'œuvre pas chère
	-> erreurs, classifications absurdes

**Au final :** rendre les datasets + juste en enlevant les catégories et termes sensibles/incorrects : pas assez pour corriger les problèmes plus profonds des systèmes de classification
	-> ne prend pas toujours pas en compte les stéréotypes et les inégalités de pouvoir

## Quels impacts de telles classifications ?

#### L'exemple de UTKFace

-> dataset de visages annotés pour âge, genre et ethnie
-  *âge :* 0 - 116
- *genre :* binaire
- *ethnie :* 5 catégories

De telles classifications racistes posent problème

-> UTKFace représente les problèmes de classifications ethniques en Afrique du Sud en 1950s : racisme en vers les gens de peau noire = mouvements restreints, interdictions de sexualité entre les ethnies, etc.
	-> BDD pour cette classification ? **IBM**, qui devait en permanence la modifier car il n'y a aucune classification ethnique "correcte"

#### Ethnie "pure"

-> grand débat, utilisé pour justifier des décisions discriminatoires à travers l'histoire
19e et 20e siècle : ethnie "pure" car les autres sont biologiquement et génétiquement inférieures

#### Prédire la sexualité/criminalité ?

-> très problématique : ces catégories ne sont pas basées sur des caractéristiques fixes mais plutôt sur des aspects complexes de l'identité humaine, façonné par la culture, le social et l'histoire
	-> renforce les préjugés/stéréotypes, intrusion de la vie privée et du droit humain, stigmatise les différences

#### Finalement

**Chaque classification a des conséquences**

Classifier, définir des catégories et les idées de "normalité" crée des formes d'anormalité, de différence, d'altérité

## Comment régler ces classifications de pouvoir et de politique ?

La justice en IA : demande + que d'optimiser les métriques ou les statistiques
	-> demande la compréhension de l'ingénierie et des mathématiques derrière ces problèmes

Nouvelle approche de classification ? 
	-> qui prendrait en compte la complexité et la fluidité des dynamiques sociales

Le problème va bien au delà déterminer si une classification est correcte ou incorrecte

Pour l'instant, évoqué que des datasets publics
Mais TikTok, Facebook, Google classifient aussi leurs utilisateurs et le fait que certains reçoivent des avantages et d'autres non, sans savoir pourquoi, demande une réponse politique collective, même si cela devient + difficile

