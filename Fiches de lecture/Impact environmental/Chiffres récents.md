## Entraînement de GPT-4

#### 1ère source

- **25 000 GPUs Nvidia A100** (enveloppe thermique 400 W chaque)
- **24 000 000 kWh** pour 100 jours d'entraînement
- 0.24 kg CO2e / kWh donc **6912 tonnes de CO2e
	- ⇔ 30 000 000 km parcourus en voiture à essence
	- ⇔ consommation énergétique de 1300 foyers pendant 1 an

#### 2ème source

- 300 tonnes de CO2 (personne moyenne : 5 tonnes par an)
- Avec *neural architecture search* : 626 000 tonnes de CO2 (⇔ durée de vie de 5 voitures américaines moyennes)
- **Modèle BERT :** ⇔ consommation d'énergie d'un vol transatlantique

## Inférence

D'après [Google](https://research.google/blog/good-news-about-the-carbon-footprint-of-machine-learning-training/), **60%** de la consommation totale d'énergie liée à l'IA est dû à l'inférence.
D'après [Nvidia](https://www.forbes.com/sites/moorinsights/2019/05/09/google-cloud-doubles-down-on-nvidia-gpus-for-inference/?sh=7aa7fce46792), ce chiffre est de **80-90%**.

#### ChatGPT avec GPT-3

100+ millions d'utilisateurs : **8,4 tonnes de CO2 par an*** (⇔ ~8 allers-retours Paris - New York)

---
Sources :

1. https://archive.md/2RQ8X
2. https://hbr.org/2023/07/how-to-make-generative-ai-greener
